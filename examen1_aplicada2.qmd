---
title: "Estadística Aplicada 3 - Examen 1"
lang: es
author: "Marcelino 191654"
date: today
header-includes:
  - \usepackage{listings}
  - \usepackage{color} % si aún no lo has incluido; es necesario para \textcolor
  - \lstset{breaklines=true, postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}}
  - \usepackage{float}

format:
  html:
    page-layout: full
    embed-resources: true
---

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# cargamos librerias y código principal
library(dplyr)
library(kableExtra)
library(dslabs)
# Cargamos bases de datos
mnist_data <- read_mnist()
data2 <- iris
source(paste0(getwd(), "/codigo1.R"))
```

\newpage

# Ejercicio 1

En este reporte se creó un clasificador con la base de datos ```MNIST``` (Modified National Institute of Standards and Technology). Se compone de un conjunto de imágenes en escala de grises de dígitos escritos a mano, del 0 al 9, y ha sido ampliamente utilizada para entrenar diversos modelos de reconocimiento de imágenes. ```MNIST``` contiene 70,000 imágenes en total, divididas en 60,000 imágenes(28x28) de entrenamiento y 10,000 imágenes de prueba. 

El objetivo de este clasificador es predecir si una imagen contenía un 1, 3 o 5. Para lograr encontrar el mejor clasificador primero se prepararon los datos de tal forma que la variable respuesta fuera de tipo factor y los regresores fueran los pixeles de la imagen. Posteriormente, se procedió a dividir la base de datos en un conjunto de entrenamiento, validación y prueba. Para esta división se utilizó el conjunto de prueba que viene con los datos y para el conjunto de validación se utilizó muestreo alteatorio estratificado con respecto a la variable ``label`` con la semilla de `191654`.  Los modelos que se utilizaron para encontrar el mejor clasificador fueron: LDA, QDA, Naive Bayes y Regresión Logística. 

Además, como la base de datos era muy grande en cuanto al número de regresores potenciales, se decidió reducir la dimensionalidad de los datos con PCA, antes de entrenar los modelos. Para esto, se realizó PCA sobre el conjunto de entrenamiento y se seleccionaron los primeros 50 componentes principales, ya que estos explicaban la mayor parte de la varianza de los datos, utilizamos Scree plot para comprobarlo, como se puede apreciar en la @fig-plot1 que se encuentra en los anexos de este reporte.

Después de entrenar cada modelo y validar métricas importantes de clasificación multiclase obtuvimos lo mostrado en @tbl-plot1 (que se encuentra en los anexos). Donde cada métrica entre más alta sea, es mejor. Para elegir el mejor modelo nos guiamos mucho más por la métrica de macro-Accuracy, la cual mide la proporción de predicciones correctas con respecto a todos los datos. Esta sería el complemento de lo que nos daría el error de clasificación. 

Con lo cual el mejor modelo de clasificación fue el modelo QDA, superior en todas las métricas. Este resultado tiene mucho sentido por lo siguiente. En primer lugar, sabemos que la regresión logística es un modelo pésimo para clasificaciones multiclase, por lo que era de esperarse que tuviera peor desempeño. En segundo, lugar notamos que estamos entrenando más de 10,000 datos los cuales sabemos que están acotados en cierto rango y con lo cual si suponemos que vienen de alguna distribución entonces tendrían todos sus momentos y por TCL cualquier estadística que sea función de la suma de estos datos se distrubirá normal, en especial tenemos que el discriminante utilizado en LDA y QDA se distribuirá normal, por lo que es de esperarse que los modelos LDA y QDA tengan cierto desempeño aunque los datos no sean normales, superando al NB porque aprovecha mejor la normalidad asintótica (recordando que en el NB que utilizamos está aproximando las densidades condicionales con una normal). Y por último, notemos que ligeramente QDA es mejor a LDA debido a que la forma en que se escribe 3 y 5 puede tener una mayor variabilidad en los pixeles de las imágenes que la forma en que se ecribe 1, por lo cual QDA describiría un poco mejor los datos que LDA. 

Por último, obviamente elegimos el QDA porque nos interesa más predicción que interpretabilidad o simplicidad.

El desempeño final de este clasificador con el conjunto de prueba se muestra en la  @tbl-plot2 y la matriz de confusión en la @fig-plot3.



\newpage

# Ejercicio 2
En esta segunda parte del reporte se realizó un análisis de clustering sobre la base de datos ```iris```.
El conjunto de datos ```iris```, introducido por el biólogo Ronald Fisher en 1936, consiste en mediciones de cuatro variables (longitud y ancho de sépalos y pétalos) de tres especies de flores iris (setosa, versicolor y virginica). Con 50 observaciones por especie, este conjunto ha sido ampliamente utilizado en estadística y aprendizaje automático como ejemplo para técnicas de análisis y clasificación debido a su claridad y tamaño manejable.

Para poder hacer cluster se calculó primero la matriz de distancias euclidianas entre las observaciones sin tomar en cuenta el dato de a qué especie pertenecía cada registro. Posteriormente se corrieron muchos algoritmos de clustering  pero se decidieron analizar los siguientes ```k-means clustering```, ```average-linkage clustering``` y ```complete-linkage clustering``` . Para el método de k-means se supuso que habían 3 clusters dado que se sabe que hay 3 especies de flores en esa base. Aunque en la vida real no sabremos cuántos clusters hay, de hecho al comparar gráficas de pares de columnas de los datos (véase la @fig-plot4 en los anexos), solo se podían apreciar bien 2 tipos de clusters bien definidos, por lo que hay que tener cuidado al elegir el número de clusters. 

```{r, echo=FALSE,  warning=FALSE, cache=TRUE, message=FALSE}
source(paste0(getwd(), "/codigo2_2.R"))
```

Los resultados de los dendogramas de los clusters jerárquicos se pueden apreciar en la @fig-plot2 que se encuentra en los anexos de este reporte. Notamos que en los dos métodos hay dos clusters bien definidos, por lo que necesariamente deben haber al menos dos clases. Sin embargo, cuando analizamos los dendogramas para medir una tercera clase, observamos que realmente no pareciera que esté bien definida por lo que podría ser que no hayan podido clusterizar bien estos métodos, sabiendo que deben haber 3 especies. Con lo cual procedimos a analizar los 3 métodos para conocer el grado de clasificación que tienen con la métrica de `accuracy`, dado que tenemos las etiquetas de los datos, y suponiendo que ciertas clusters se refierían a ciertas etiquetas. Y obtuvimos los datos que se muestran en la tabla.

```{r, echo=FALSE,  warning=FALSE, message=FALSE}
library(kableExtra)
# Generar tabla para RMarkdown
kable(results_df, format = "latex", booktabs = TRUE) |> 
  kable_styling(latex_options = "H")

```

Por lo tanto, notamos que el mejor método fue ```average-linkage``` y ligeramente mejor que el ```knn```. Esto tiene sentido, porque aunque se vio que era difícil de clasificar tanto por lo visto en los dendogramas como por lo visto en las imágenes de datos, estos métodos podrían sortear mejor este problema, a comparación del ```single-linkage``` que podría desviarse fácilmente al irse a los extremos en sus criterior de separación.  Además, hay que reconocer que el ```average-linkage``` fuera ligeramente mejor sin conocer el número de clusters previamente.

\newpage

# Anexos

## Tablas

```{r, echo=FALSE,  warning=FALSE, label="tbl-plot1"}
#| tbl-cap: Resumen de resultados
library(kableExtra)
# Mostrar el dataframe con los resultados
kable(resultados, format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = c("scale_down", "H"))

```

```{r, echo=FALSE, warning=FALSE, label="tbl-plot2", message=FALSE}
#| tbl-cap: Resumen de resultados QDA
# Mostrar el dataframe con los resultados
library(dplyr)
library(kableExtra)
kable(resultados_test, format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("scale_down", "H"))

```

## Figuras

```{r, echo=FALSE, warning=FALSE, cache=TRUE, label="fig-plot1"}
#| fig.align='center',
#| out.width="70%",
#| fig.pos='H',
#| fig.cap="Scree plot para seleccionar el número de componentes principales a utilizar en PCA."


ggplot(filtered_data, aes(x = component, y = value)) +
  geom_line() +
  geom_point() +
  labs(title = "Scree plot",
    x = "Principal Component",
       y = "Varianza explicada") +
  theme_minimal() +
  geom_vline(aes(xintercept=50), color="red", linetype="dashed")+
  annotate("text", x = 50, y = min(filtered_data$value), label = "x=50", vjust = -2, hjust = -1, color="red") +
  theme(plot.title = element_text(hjust = 0.5))

```

```{r, echo=FALSE,  warning=FALSE, cache=TRUE, label="fig-plot3"}
#| out.width="70%",
#| fig.pos='H',
#| fig.cap="Matriz de confusión para el modelo QDA."

workflowq <- workflow() %>%
    add_recipe(rec) %>%
    add_model(qda_spec)
fit <- fit(workflowq, data = train_data)
# Matriz de confusión
augment(fit, new_data = test_data) |>
  conf_mat(truth = label, estimate = .pred_class) |>
  autoplot(type = "heatmap")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE,  fig.position="H",label="fig-plot2"}
#| fig-cap: "Dendogramas de los algoritmos de clustering"
#| layout-ncol: 2
#| fig-subcap: 
#|   - "Average Linkage"
#|   - "Complete Linkage"

# Dendogramas

plot(hclust_average)
plot(hclust_complete)

```

```{r, echo=FALSE,message=FALSE,  warning=FALSE, cache=TRUE, label="fig-plot4"}
#| fig.align='center',
#| out.width="100%",
#| fig.pos='H',
#| fig.cap="Comparación de columnas a pares de los datos de Iris"

library(ggplot2)
library(gridExtra)

# Extraer las columnas numéricas
data_num <- iris[, 1:4]

# Crear una lista de plots para cada combinación de características
plots_list <- list()

for (i in 1:4) {
  for (j in 1:4) {
    if (i != j) {
      p <- ggplot(iris, aes_string(names(data_num)[i], names(data_num)[j], color = "Species")) +
        geom_point(size = 2, alpha = 0.7) +
        labs(x = names(data_num)[i], y = names(data_num)[j]) +
        theme_minimal()
      
      plots_list[[paste0(names(data_num)[i], "_vs_", names(data_num)[j])]] <- p
    }
  }
}

# Mostrar los plots en una matriz
grid.arrange(grobs = plots_list, ncol = 3)

```
