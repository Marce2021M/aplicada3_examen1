---
title: "Estadística Aplicada 3 - Examen 1"
lang: es
author: "Marcelino"
date: today
header-includes:
  - \usepackage{listings}
  - \usepackage{color} % si aún no lo has incluido; es necesario para \textcolor
  - \lstset{breaklines=true, postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}}

format:
  html:
    page-layout: full
    embed-resources: true
---

```{r, message=FALSE, warning=FALSE}
#Cargamos paquetes
library(tidymodels)
library(discrim)
library(corrr)
library(paletteer)
library(MASS)
library(dslabs)
library(tidyr)

# Cargamos bases de datos
mnist_data <- read_mnist()
data2 <- iris
```

# Ejercicio 1

En este reporte se creó un clasificador con la base de datos ```MNIST``` (Modified National Institute of Standards and Technology), esta base es uno de los conjuntos de datos más icónicos en el campo del aprendizaje automático y la visión por computadora. Se compone de un conjunto de imágenes en escala de grises de dígitos escritos a mano, del 0 al 9, y ha sido ampliamente utilizada para entrenar diversos modelos de reconocimiento de imágenes. MNIST contiene 70,000 imágenes en total, divididas en 60,000 imágenes de entrenamiento y 10,000 imágenes de prueba. Cada imagen tiene un tamaño de 28x28 píxeles, lo que da un total de 784 píxeles por imagen. 

El objetivo de este clasificador era predecir si una imagen contenía un 1, 3 o 5. Para lograr encontrar el mejor clasificador primero se prepararon los datos de tal forma que la variable respuesta fuera de tipo factor y los regresores fueran los pixeles de la imagen. Posteriormente, se procedió a dividir la base de datos en un conjunto de entrenamiento, validación y prueba. El conjunto de entrenamiento y validación se utilizó para encontrar el mejor modelo y el conjunto de prueba para evaluar el desempeño del modelo. Además, como la base de datos era muy grande en cuanto al número de regresores potenciales, se decidió reducir la dimensionalidad de los datos con PCA.



 Los modelos que se utilizaron para encontrar el mejor clasificador fueron: LDA, QDA, Naive Bayes y Regresión Logística. 


```{r,  message=FALSE, warning=FALSE, cache=TRUE}
#Extraer el train y test

## Preparamos los datos

### Entrenamiento y validacion
flattened_images <- matrix(mnist_data$train$images, nrow = dim(mnist_data$train$images)[1], ncol = 28*28)
df <- data.frame(label = mnist_data$train$labels)
df <- cbind(df, flattened_images)

### Testeo

flattened_images2 <- matrix(mnist_data$test$images, nrow = dim(mnist_data$test$images)[1], ncol = 28*28)
test_data <- data.frame(label = mnist_data$test$labels)
test_data <- cbind(test_data, flattened_images2)

## Transformar el train y test al estadístico que deseamos de solo 1, 3 y 5

df <- df  |> filter(label == 1 | label == 3 | label == 5)

test_data <- test_data |> filter(label == 1 | label == 3 | label == 5) # data testeo
test_data$label <- as.factor(test_data$label)

# Spliteamos datos de entrenamiento y validación para elegir el mejor modelo

set.seed(191654)
### Spliteamos la data para validación y entrenamiento
data_split <- rsample::initial_split(df, prop = .8, strata = "label")

train_data <- training(data_split) # data train
train_data$label <- as.factor(train_data$label)

val_data <- testing(data_split) # data validación
val_data$label <- as.factor(val_data$label)

# Preparación de datos para reducir dimensionalidad con PCA

# Preparamos motores de modelo

lda_spec <- discrim_linear() |>
  set_mode("classification") |>
  set_engine("MASS")

qda_spec <- discrim_quad() |>
  set_mode("classification") |>
  set_engine("MASS")

nb_spec <- naive_Bayes() |> 
  set_mode("classification") |> 
  set_engine("klaR") |> 
  set_args(usekernel = FALSE)

lr_spec <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification")


# Preparamos recetas

rec <- recipe(label ~ ., data = train_data) %>%
    #step_zv(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_pca(all_predictors(), num_comp=100)

prep_rec <- prep(rec, training = train_data)

train_data_pca <- bake(prep_rec, new_data = train_data)

# Get variance explained by each component
pca_variance <- tidy(prep_rec, number = 3, type = "variance")

filtered_data <- pca_variance[pca_variance$terms == "variance", ]
library(ggplot2)

ggplot(filtered_data, aes(x = component, y = value)) +
  geom_line() +
  geom_point() +
  labs(title = "Scree Plot",
       x = "Principal Component",
       y = "Cumulative Percent Variance Explained") +
  theme_minimal() +
  geom_vline(aes(xintercept=50), color="red", linetype="dashed")

#arreglamos receta a 50 componentes

rec <- recipe(label ~ ., data = train_data) %>%
    #step_zv(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_pca(all_predictors(), num_comp=50)

# Preparamos workflows

all_workflows <- workflow_set(
  preproc = list(receta =rec ), 
  models = list(LDA = lda_spec, QDA = qda_spec, NB = nb_spec, LR = lr_spec)
)

# Conjuntos de cross validation

cv <- vfold_cv(train_data, v = 7, strata = "label")

# Grid de hiperparámetros
grid_ctrl <-
  control_grid(
    save_pred = TRUE,
    parallel_over = "resamples",
    save_workflow = TRUE,
    verbose=TRUE,

  )

all_cores <- parallel::detectCores(logical = TRUE) - 3
library(doParallel)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)

system.time(
  grid_results <- all_workflows |>
    workflow_map(
      seed = 191654,
      resamples = cv,
      control = grid_ctrl,
      verbose=TRUE,
      metrics=metric_set(recall, precision, accuracy, f_meas, roc_auc)
    )
)
grid_results <- readRDS("grid_results.rds")

### Estracción mejor modelo

#NB

best_resultsNBAYES <- 
  grid_results |> 
  extract_workflow_set_result("receta_NB") |> 
  select_best(metric = "roc_auc")

best_resultsNBAYES

NBAYES_test_results <- 
   grid_results |> 
   extract_workflow("receta_NB") |> 
   finalize_workflow(best_resultsNBAYES) |> 
   fit(base_train)

#LR

best_resultsLR <- 
  grid_results |> 
  extract_workflow_set_result("receta_LR") |> 
  select_best(metric = "roc_auc")

best_resultsLR

LR_test_results <- 
   grid_results |> 
   extract_workflow("receta_LR") |> 
   finalize_workflow(best_resultsLR) |> 
   fit(base_train)#NB


#QDA

best_resultsQDA <- 
  grid_results |> 
  extract_workflow_set_result("receta_QDA") |> 
  select_best(metric = "roc_auc")

best_resultsQDA

QDA_test_results <- 
   grid_results |> 
   extract_workflow("receta_QDA") |> 
   finalize_workflow(best_resultsQDA) |> 
   fit(base_train)

#LDA

best_resultsLDA <- 
  grid_results |> 
  extract_workflow_set_result("receta_LDA") |> 
  select_best(metric = "roc_auc")

best_resultsLDA

LDA_test_results <- 
   grid_results |> 
   extract_workflow("receta_LDA") |> 
   finalize_workflow(best_resultsLDA) |> 
   fit(base_train)



```



# Ejercicio 2

```{r}
library(cluster)

#Iris DB
data <- as.matrix(iris[,1:4])
dist_mat <- dist(data, method = 'euclidean')
```


```{r}
hclust_single <- hclust(dist_mat, method = 'single')
hclust_average <- hclust(dist_mat, method='average')
hclust_complete <- hclust(dist_mat, method= 'complete')
divisive_model <- agnes(dist_mat, method = "single")
```